{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create placeholders for X and y\n",
    "X = tf.placeholder(name='X', shape=(None,28,28,1), dtype=tf.float32)\n",
    "y = tf.placeholder(name='y', shape=(None,10), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize weights and bias\n",
    "#using xavier initialization for weights and zero initialization for bias\n",
    "W1 = tf.get_variable(name='W1', shape=(5,5,1,6), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "W2 = tf.get_variable(name='W2', shape=(5,5,6,16), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "W3 = tf.get_variable(name='W3', shape=(400,120), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "W4 = tf.get_variable(name='W4', shape=(120,84), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "W5 = tf.get_variable(name='W5', shape=(84,10), dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "B1 = tf.get_variable(name='B1', shape=(6), dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "B2 = tf.get_variable(name='B2', shape=(16), dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "B3 = tf.get_variable(name='B3', shape=(120), dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "B4 = tf.get_variable(name='B4', shape=(84), dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "B5 = tf.get_variable(name='B5', shape=(10), dtype=tf.float32, initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv_layers/Relu:0\", shape=(?, 28, 28, 6), dtype=float32)\n",
      "Tensor(\"conv_layers/MaxPool:0\", shape=(?, 14, 14, 6), dtype=float32)\n",
      "Tensor(\"conv_layers/Relu_1:0\", shape=(?, 10, 10, 16), dtype=float32)\n",
      "Tensor(\"conv_layers/MaxPool_1:0\", shape=(?, 5, 5, 16), dtype=float32)\n",
      "Tensor(\"fc_layers/Flatten/flatten/Reshape:0\", shape=(?, 400), dtype=float32)\n",
      "Tensor(\"fc_layers/Relu:0\", shape=(?, 120), dtype=float32)\n",
      "Tensor(\"fc_layers/Relu_1:0\", shape=(?, 84), dtype=float32)\n",
      "Tensor(\"fc_layers/add_2:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"output/Softmax:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#CNN model - A 'modern' LeNet-5\n",
    "#\n",
    "#conv -> relu -> max pooling -> conv -> relu -> FC -> relu -> FC -> relu -> FC -> softmax\n",
    "#\n",
    "#input - m * 28 * 28 *1  \n",
    "#\n",
    "#convolutional layer 1 (relu) - 28 * 28 * 1 -> 28 * 28 * 6     stride=1, filter_size=5\n",
    "#max pooling layer 1 - 28 * 28 * 6 -> 14 * 14 * 6              stride=2, filter_size=2\n",
    "#\n",
    "#convolutional layer 2 (relu) - 14 * 14 * 6 -> 10 * 10 * 16    stride=1, filter_size=5\n",
    "#max pooling layer 2 - 10 * 10 * 16 -> 5 * 5 * 16              stride=2, filter_size=2\n",
    "#\n",
    "#flatten - 5 * 5 * 16 -> 400\n",
    "#\n",
    "#full-connected layer 1 (relu) - 400 -> 120\n",
    "#\n",
    "#full-connected layer 2 (relu) - 120 ->84\n",
    "#\n",
    "#full-connected layer 3 (softmax) - 84 -> 10\n",
    "\n",
    "#use tf.name_scope to make you visualization more organized.\n",
    "with tf.name_scope('conv_layers') as scope:\n",
    "        A1 = tf.nn.relu(tf.nn.conv2d(input=X, filter=W1, strides=(1,1,1,1), padding='SAME')+B1)\n",
    "        print(A1)\n",
    "        P1 = tf.nn.max_pool(value=A1, ksize=(1,2,2,1), strides=(1,2,2,1), padding='SAME')\n",
    "        print(P1)\n",
    "        A2 = tf.nn.relu(tf.nn.conv2d(input=P1, filter=W2, strides=(1,1,1,1), padding='VALID')+B2)\n",
    "        print(A2)\n",
    "        P2 = tf.nn.max_pool(value=A2, ksize=(1,2,2,1), strides=(1,2,2,1), padding='SAME')\n",
    "        print(P2)\n",
    "with tf.name_scope('fc_layers') as scope:\n",
    "        P2_flatten = tf.contrib.layers.flatten(P2)\n",
    "        print(P2_flatten)\n",
    "        A3 = tf.nn.relu(tf.matmul(P2_flatten, W3) + B3)\n",
    "        print(A3)\n",
    "        A4 = tf.nn.relu(tf.matmul(A3, W4) + B4)\n",
    "        print(A4)\n",
    "        Z5 = tf.matmul(A4, W5) + B5\n",
    "        print(Z5)\n",
    "with tf.name_scope('output'):\n",
    "        y_pred = tf.nn.softmax(Z5)\n",
    "        print(y_pred)\n",
    "\n",
    "#compute cost\n",
    "with tf.name_scope('loss') as scope:\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z5, labels=y))\n",
    "    \n",
    "    #Since we won't actually run this graph , we don't need to use the method 'summary', but if you want to keep track \n",
    "    #something while training, it is useful to use 'summary'\n",
    "    tf.summary.scalar(\"cost_function\", loss)\n",
    "#define a step\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    step = tf.train.AdamOptimizer(learning_rate=0.006).minimize(loss)\n",
    "with tf.name_scope('accuracy') as scope:    \n",
    "    #prediction\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n",
    "    #compute accuracy    #use this to compute train or test accuracy.\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "#merge all summaries\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_graph\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    #file will be written to a folder named 'the_graph'\n",
    "    writer = tf.summary.FileWriter('the_graph', sess.graph)\n",
    "    writer.add_graph(graph=sess.graph)\n",
    "    \n",
    "    #For summary, we will use something like\n",
    "    \n",
    "    #summary,... = sess.run([merged_summary,...],...)\n",
    "    #writer.add_summary(summary,...)\n",
    "    \n",
    "    print(writer.get_logdir())\n",
    "    \n",
    "    #just make sure everything is working\n",
    "    writer.flush()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to rerun the code above, don't forget to clear up the previous graph first\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在命令行模式中键入以下代码：\n",
    "\n",
    "tensorboard --logdir=你储存的文件夹路径名\n",
    "\n",
    "注意：使用绝对路径可以保证读取到，不要引号，不要引号，不要引号！！！文件夹名字即上面print出来的结果。之后根据提示打开网页即可。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
